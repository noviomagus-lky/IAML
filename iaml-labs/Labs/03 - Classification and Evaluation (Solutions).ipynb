{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Support Vector Machine (SVM) Classification and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we initially re-examine the spam filtering problem from Lab 2. This time, we train a Logistic Regression model and a linear Support Vector Machine for the spam or non-spam classification task. In the second part of the lab we examine classification evaluation by using a K-nearest neighbour classifier.\n",
    "\n",
    "\n",
    "All the datasets that you will need for this lab are located within the `datasets` directory (adjacent to this file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "from pandas.api.types import CategoricalDtype\n",
    "KNeighboursClassifier = KNeighborsClassifier # For the Brits!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 ==========\n",
    "Load `spambase_binary.csv` into a pandas DataFrame structure called `spambase`. Display the number of instances and attributes and the first 5 samples. Remember that the attributes have been binarised. The instances have also been shuffled (i.e. their order has been randomised). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 4601, number of attributes: 55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make_binarized</th>\n",
       "      <th>word_freq_address_binarized</th>\n",
       "      <th>word_freq_all_binarized</th>\n",
       "      <th>word_freq_3d_binarized</th>\n",
       "      <th>word_freq_our_binarized</th>\n",
       "      <th>word_freq_over_binarized</th>\n",
       "      <th>word_freq_remove_binarized</th>\n",
       "      <th>word_freq_internet_binarized</th>\n",
       "      <th>word_freq_order_binarized</th>\n",
       "      <th>word_freq_mail_binarized</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_edu_binarized</th>\n",
       "      <th>word_freq_table_binarized</th>\n",
       "      <th>word_freq_conference_binarized</th>\n",
       "      <th>char_freq_;_binarized</th>\n",
       "      <th>char_freq_(_binarized</th>\n",
       "      <th>char_freq_[_binarized</th>\n",
       "      <th>char_freq_!_binarized</th>\n",
       "      <th>char_freq_$_binarized</th>\n",
       "      <th>char_freq_#_binarized</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make_binarized  word_freq_address_binarized  \\\n",
       "0                         0                            1   \n",
       "1                         0                            0   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "4                         0                            0   \n",
       "\n",
       "   word_freq_all_binarized  word_freq_3d_binarized  word_freq_our_binarized  \\\n",
       "0                        0                       0                        1   \n",
       "1                        0                       0                        0   \n",
       "2                        1                       0                        0   \n",
       "3                        1                       0                        1   \n",
       "4                        0                       0                        1   \n",
       "\n",
       "   word_freq_over_binarized  word_freq_remove_binarized  \\\n",
       "0                         0                           1   \n",
       "1                         0                           0   \n",
       "2                         0                           0   \n",
       "3                         0                           0   \n",
       "4                         0                           0   \n",
       "\n",
       "   word_freq_internet_binarized  word_freq_order_binarized  \\\n",
       "0                             1                          0   \n",
       "1                             0                          0   \n",
       "2                             0                          1   \n",
       "3                             0                          0   \n",
       "4                             0                          0   \n",
       "\n",
       "   word_freq_mail_binarized  ...  word_freq_edu_binarized  \\\n",
       "0                         1  ...                        0   \n",
       "1                         0  ...                        1   \n",
       "2                         0  ...                        0   \n",
       "3                         0  ...                        0   \n",
       "4                         0  ...                        0   \n",
       "\n",
       "   word_freq_table_binarized  word_freq_conference_binarized  \\\n",
       "0                          0                               0   \n",
       "1                          0                               0   \n",
       "2                          0                               0   \n",
       "3                          0                               0   \n",
       "4                          0                               0   \n",
       "\n",
       "   char_freq_;_binarized  char_freq_(_binarized  char_freq_[_binarized  \\\n",
       "0                      0                      1                      1   \n",
       "1                      0                      0                      0   \n",
       "2                      0                      0                      0   \n",
       "3                      0                      0                      0   \n",
       "4                      0                      1                      0   \n",
       "\n",
       "   char_freq_!_binarized  char_freq_$_binarized  char_freq_#_binarized  \\\n",
       "0                      1                      1                      0   \n",
       "1                      1                      0                      0   \n",
       "2                      0                      0                      0   \n",
       "3                      1                      0                      0   \n",
       "4                      1                      1                      0   \n",
       "\n",
       "   is_spam  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'spambase_binary.csv')\n",
    "spambase = pd.read_csv(data_path, delimiter = ',')\n",
    "print('Number of instances: {}, number of attributes: {}'.format(spambase.shape[0], spambase.shape[1]))\n",
    "spambase.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 ==========\n",
    "We are going to use hold-out validation to evaluate our models below. Split the dataset into training and testing subsets using the `train_test_split` [function](http://scikit-learn.org/0.19/modules/generated/sklearn.cross_validation.train_test_split.html) we have used before. Call the resulting matrices `X_train`, `X_test`, `y_train`, `y_test`. Use 90% of the data for training and the remaining 10% for testing. Make sure you don't include the target variable `is_spam` in the input features (`X_train` / `X_test`)!\n",
    "\n",
    "If you want to be able to reproduce your results exactly, what argument must you remember to set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student needs to provide code similar to below\n",
    "X_train, X_test, y_train, y_test = train_test_split(spambase.drop(\"is_spam\", axis=1), spambase[\"is_spam\"],\n",
    "                                                    train_size=0.9, test_size=0.1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "You must set the `random_state` argument, otherwise, every time you run the `train_test_split` function, you will get a different random split of the data. This will give you different results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 ==========\n",
    "Train a [`LogisticRegression`](http://scikit-learn.org/0.19/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier by using training data. Use the `lbfgs` solver and default settings for the other parameters. Report the classification accuracy on both the training and test sets. Does your classifier generalise well on unseen data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on training set: 0.935\n",
      "Classification accuracy on test set: 0.928\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, y_train)\n",
    "print('Classification accuracy on training set: {:.3f}'.format(lr.score(X_train, y_train)))\n",
    "print('Classification accuracy on test set: {:.3f}'.format(lr.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "Indeed, the classifier generalises well since the classification accuracy on the test set is comparable to the training classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 ==========\n",
    "Print the coefficients for class 1 for the attributes `word_freq_hp_binarized` and `char_freq_$_binarized`. Generally, we would expect the string `$` to appear in spam, and the string `hp` to appear in non-spam e-mails, as the data was collected from HP Labs. Do the regression coefficients make sense given that class 1 is spam? *Hint: Consider the sigmoid function and how it transforms values into a probability between 0 and 1. Since our attributes are boolean, a positive coefficient can only increase the total sum fed through the sigmoid and thus move the output of the sigmoid towards 1. What can happen if we have continuous, real-valued attributes?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient for word_freq_hp_binarized: -2.639\n",
      "Coefficient for char_freq_$_binarized: 1.699\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "hp_ind = spambase.columns.get_loc('word_freq_hp_binarized')\n",
    "dollar_ind = spambase.columns.get_loc('char_freq_$_binarized')\n",
    "print('Coefficient for word_freq_hp_binarized: {:.3f}'.format(lr.coef_[0, hp_ind]))\n",
    "print('Coefficient for char_freq_$_binarized: {:.3f}'.format(lr.coef_[0, dollar_ind]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "The coefficients make sense, since the attribute `word_freq_hp_binarized` has a negative coefficient, meaning that it provides support for `non-spam` classification. On the other hand, `char_freq_$_binarized` has a positive coefficient, thus providing support for class 1 (i.e. `spam`).\n",
    "\n",
    "With continuous valued input, in particular if it can go negative, a positive coefficient may not necessarily mean that it will shift the output towards 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 ==========\n",
    "Train a [`LinearSVC`](http://scikit-learn.org/0.19/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (i.e. Linear Support Vector classifier) by using default parameters. Report the classification accuracy on the training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC classification accuracy on training set: 0.935\n",
      "Linear SVC classification accuracy on test set: 0.920\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "svc_linear = LinearSVC()\n",
    "svc_linear.fit(X_train, y_train)\n",
    "print('Linear SVC classification accuracy on training set: {:.3f}'.format(svc_linear.score(X_train, y_train)))\n",
    "print('Linear SVC classification accuracy on test set: {:.3f}'.format(svc_linear.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.6 ==========\n",
    "What are the coefficients for the attributes `word_freq_hp_binarized` and `char_freq_`$`_binarized`? Compare these to the ones you found with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient for word_freq_hp_binarized: -0.857\n",
      "Coefficient for char_freq_$_binarized: 0.569\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "print('Coefficient for word_freq_hp_binarized: {:.3f}'.format(svc_linear.coef_[0, hp_ind]))\n",
    "print('Coefficient for char_freq_$_binarized: {:.3f}'.format(svc_linear.coef_[0, dollar_ind]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "They are consistent: however, they are also less pronounced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.7 ==========\n",
    "How does a linear SVM relate to Logistic Regression? *Hint: Consider the classification boundary learnt in each model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "They both have linear classification boundaries with respect to the inputs. The classification mechanism is, however, fundamentally different. Logistic regression applies a linear transformation to the input and the result is transformed to a probability by using a non-linear function. Support vector machine classification is based on the similiarty of a new data point to a few training instances known as support vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.8 ==========\n",
    "By using the [`SVC`](http://scikit-learn.org/0.19/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) class train two new support vector classifiers with Gaussian (`rbf`) and polynomial (`poly`) kernels. Again, report classification accuracies on training and test sets and compare with your results from Question 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/inf.ed.ac.uk/user/s18/s1891130/miniconda3/envs/iamlassignment2/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/afs/inf.ed.ac.uk/user/s18/s1891130/miniconda3/envs/iamlassignment2/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF SVC classification accuracy on training set: 0.934\n",
      "RBF SVC classification accuracy on test set: 0.931\n",
      "Poly SVC classification accuracy on training set: 0.873\n",
      "Poly SVC classification accuracy on test set: 0.874\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "svc_rbf = SVC(kernel='rbf')\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "svc_poly = SVC(kernel='poly', degree=2)\n",
    "svc_poly.fit(X_train, y_train)\n",
    "print('RBF SVC classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X_train, y_train)))\n",
    "print('RBF SVC classification accuracy on test set: {:.3f}'.format(svc_rbf.score(X_test, y_test)))\n",
    "print('Poly SVC classification accuracy on training set: {:.3f}'.format(svc_poly.score(X_train, y_train)))\n",
    "print('Poly SVC classification accuracy on test set: {:.3f}'.format(svc_poly.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "It appears that there is a very slight benefit to using a radial basis kernel. However, before we can make any solid claims, we need to perform a cross-validation and try different parameters for these kernels. For an example, see [the sklearn doc examples](http://scikit-learn.org/0.19/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Performance assessment\n",
    "We will now look at a few ways of assessing the performance of a classifier. To do so we will introduce a new data set, the [Splice](https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29) data set. The classification task is to identify `intron` and `exon` boundaries on gene sequences. For more information, you can read the dataset description in the link. The class attribute can take on 3 values: `N`, `IE` and `EI`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.1 ==========\n",
    "Load the `splice_train.csv` and `splice_test.csv` into two separate dataframes. Display the shape and first 10 instances for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 2935, number of attributes: 61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos0</th>\n",
       "      <th>pos1</th>\n",
       "      <th>pos2</th>\n",
       "      <th>pos3</th>\n",
       "      <th>pos4</th>\n",
       "      <th>pos5</th>\n",
       "      <th>pos6</th>\n",
       "      <th>pos7</th>\n",
       "      <th>pos8</th>\n",
       "      <th>pos9</th>\n",
       "      <th>...</th>\n",
       "      <th>pos51</th>\n",
       "      <th>pos52</th>\n",
       "      <th>pos53</th>\n",
       "      <th>pos54</th>\n",
       "      <th>pos55</th>\n",
       "      <th>pos56</th>\n",
       "      <th>pos57</th>\n",
       "      <th>pos58</th>\n",
       "      <th>pos59</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos0 pos1 pos2 pos3 pos4 pos5 pos6 pos7 pos8 pos9  ... pos51 pos52 pos53  \\\n",
       "0    T    G    A    T    G    C    C    T    G    C  ...     C     C     C   \n",
       "1    G    C    C    C    A    T    A    T    T    C  ...     T     G     G   \n",
       "2    G    G    C    T    G    C    C    G    G    A  ...     A     C     T   \n",
       "3    C    T    G    C    T    G    C    T    G    G  ...     G     G     C   \n",
       "4    T    C    C    C    C    G    A    G    C    C  ...     A     T     C   \n",
       "5    A    T    A    C    C    T    G    C    C    C  ...     A     T     G   \n",
       "6    T    T    C    T    C    C    A    T    T    T  ...     G     A     T   \n",
       "7    A    A    A    G    A    T    G    A    T    A  ...     A     A     G   \n",
       "8    C    C    A    A    T    C    C    C    A    G  ...     G     G     C   \n",
       "9    G    C    C    G    T    G    G    T    T    T  ...     A     A     G   \n",
       "\n",
       "  pos54 pos55 pos56 pos57 pos58 pos59 class  \n",
       "0     C     C     T     G     A     G     N  \n",
       "1     A     C     T     T     C     C     N  \n",
       "2     G     T     G     T     C     T    EI  \n",
       "3     T     G     C     T     G     G    EI  \n",
       "4     A     G     C     G     C     A     N  \n",
       "5     G     G     G     T     C     T    EI  \n",
       "6     A     T     C     C     A     T    IE  \n",
       "7     C     C     C     T     T     C    EI  \n",
       "8     G     G     C     C     T     G     N  \n",
       "9     G     C     T     C     C     T    EI  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Splice Train Here\n",
    "\n",
    "# Student needs to provide code similar to below\n",
    "data_path_train = os.path.join(os.getcwd(), 'datasets', 'splice_train.csv')\n",
    "splice_train = pd.read_csv(data_path_train, delimiter = ',')\n",
    "print('Number of instances: {}, number of attributes: {}'.format(splice_train.shape[0], splice_train.shape[1]))\n",
    "splice_train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 255, number of attributes: 61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos0</th>\n",
       "      <th>pos1</th>\n",
       "      <th>pos2</th>\n",
       "      <th>pos3</th>\n",
       "      <th>pos4</th>\n",
       "      <th>pos5</th>\n",
       "      <th>pos6</th>\n",
       "      <th>pos7</th>\n",
       "      <th>pos8</th>\n",
       "      <th>pos9</th>\n",
       "      <th>...</th>\n",
       "      <th>pos51</th>\n",
       "      <th>pos52</th>\n",
       "      <th>pos53</th>\n",
       "      <th>pos54</th>\n",
       "      <th>pos55</th>\n",
       "      <th>pos56</th>\n",
       "      <th>pos57</th>\n",
       "      <th>pos58</th>\n",
       "      <th>pos59</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>IE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos0 pos1 pos2 pos3 pos4 pos5 pos6 pos7 pos8 pos9  ... pos51 pos52 pos53  \\\n",
       "0    C    C    C    T    C    C    C    A    C    T  ...     C     C     C   \n",
       "1    C    A    C    T    G    A    G    T    T    G  ...     G     A     A   \n",
       "2    C    A    G    A    C    T    G    G    G    T  ...     A     G     A   \n",
       "3    A    G    T    G    A    T    T    G    A    C  ...     T     A     C   \n",
       "4    G    T    A    G    A    C    A    C    C    T  ...     A     T     C   \n",
       "5    C    T    T    G    T    T    A    C    A    G  ...     C     C     G   \n",
       "6    C    G    T    C    A    A    T    C    A    A  ...     A     A     A   \n",
       "7    G    T    C    C    G    T    G    C    C    T  ...     G     C     C   \n",
       "8    A    T    A    C    C    T    G    T    A    G  ...     C     G     T   \n",
       "9    G    G    T    G    G    G    C    C    A    A  ...     C     A     G   \n",
       "\n",
       "  pos54 pos55 pos56 pos57 pos58 pos59 class  \n",
       "0     A     G     T     G     C     A    IE  \n",
       "1     C     C     A     G     T     G     N  \n",
       "2     C     C     A     C     A     G    EI  \n",
       "3     C     A     A     A     G     A     N  \n",
       "4     C     C     T     T     C     T    IE  \n",
       "5     A     G     A     A     C     C     N  \n",
       "6     A     T     T     A     A     G    EI  \n",
       "7     C     T     T     T     G     C     N  \n",
       "8     T     T     A     T     A     T     N  \n",
       "9     G     C     A     T     G     G     N  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Splice Test Here\n",
    "\n",
    "# Student needs to provide code similar to below\n",
    "data_path_test = os.path.join(os.getcwd(), 'datasets', 'splice_test.csv')\n",
    "splice_test = pd.read_csv(data_path_test, delimiter = ',')\n",
    "print('Number of instances: {}, number of attributes: {}'.format(splice_test.shape[0], splice_test.shape[1]))\n",
    "splice_test.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.2 ========== \n",
    "Convert the categorical attributes into numeric ones by using the [`get_dummies(...)`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.get_dummies.html) function from pandas. Make sure to take care of the values `D`, `N`, `S`, `R` (see the [documentation](https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Splice-junction+Gene+Sequences%29) for the data). *Hint: checkout the pandas [`CategoricalDtype`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.api.types.CategoricalDtype.html#pandas.api.types.CategoricalDtype)*. Also, make sure to not transform the target variable (`class`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student needs to provide code similar to below\n",
    "ctype = CategoricalDtype(['A', 'G', 'T', 'C', 'D', 'N', 'R', 'S'])\n",
    "splice_train_1hot = pd.get_dummies(splice_train.drop('class', axis=1).astype(ctype))\n",
    "splice_test_1hot = pd.get_dummies(splice_test.drop('class', axis=1).astype(ctype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.3 ==========\n",
    "Store the training and testing data into numpy arrays `X_train`, `y_train`, `X_test` and `y_test`. Display the shapes of the four arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2935, 480)\n",
      "y_train shape: (2935,)\n",
      "X_train shape: (255, 480)\n",
      "y_test shape: (255,)\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "X_train = splice_train_1hot.values\n",
    "X_test = splice_test_1hot.values\n",
    "y_train = splice_train[\"class\"]\n",
    "y_test = splice_test[\"class\"]\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_train shape: {}'.format(X_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.4 ==========\n",
    "Familiarise yourself with [Nearest Neighbours Classification](http://scikit-learn.org/0.19/modules/neighbors.html#classification). Use a [`KNeighborsClassifier`](http://scikit-learn.org/0.19/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "by using a single neighbour. Report the classification accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classification (k=1) accuracy on training set: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "print('KNN classification (k=1) accuracy on training set: {:.3f}'.format( knn.score(X_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.5 ==========\n",
    "Is the above result meaningful? Why is testing on the training data a particularly bad idea for a 1-nearest neighbour classifier? Do you expect the performance of the classifier on a test set to be as good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "The above result is not meaningful. The 1-nearest neighbour classifier will classify each instace to the class of the nearest insance in the training set. If we test the classifier on the training set, then each input will be classified correctly since the closest instance in the training set is the instance itself. The performance on unseen data is expected to be much lower.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.6 ==========\n",
    "Now report the classification accuracy on the test set and check your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN classification (k=1) accuracy on test set: 0.745\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "print('KNN classification (k=1) accuracy on test set: {:.3f}'.format(knn.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.7 ==========\n",
    "Plot a histogram of the target variable (i.e. `class`) in the test set. *Hint: matplotlib won't allow you to plot a histogram for categorical values. Instead, you can use Pandas' built-in bar plot tool in conjunction with the [`value_counts`](http://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Series.value_counts.html).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "class_counts = splice_test[\"class\"].value_counts()\n",
    "ax = class_counts.plot(kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.8 ==========\n",
    "What would be the accuracy of the classifier, if all points were labelled as `N`? \n",
    "\n",
    "**Pro Tip** - You should always use a ['Dummy Model'](http://scikit-learn.org/0.19/modules/model_evaluation.html#dummy-estimators) (a ridiculously simple model) like this to compare with your 'real' models. It's very common for complex models to be outperformed by a simple model, such as predicting the most common class. When complex models are outperformed by 'Dummies', you should investigate why: often there was an issue with the code, the data, or the way the model works was misunderstood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline classifier (predict always N) would achieve a classification accuracy score of: 0.584\n"
     ]
    }
   ],
   "source": [
    "# Student needs to provide code similar to below\n",
    "print('The baseline classifier (predict always N) would achieve a classification accuracy score of: {:.3f}'.\n",
    "      format(class_counts[\"N\"] / class_counts.values.sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.9 ==========\n",
    "Now we want to explore the effect of the `k` parameter. To do this, train the classifier multiple times, each time setting the KNN option to a different value. Try `5`, `10`, `50`, `100`, `200`, `500`, `1000`, `1500` and `2000` and test the classifier on the test set. How does the k parameter effect the results? *Hint: Consider how well the classifier is generalising to previously unseen data, and how it compares to the dumb prediction accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student needs to provide code similar to below\n",
    "k_list = [5, 10, 50, 100, 200, 500, 1000, 1500, 2000]\n",
    "ca = []\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    score = knn.score(X_test,y_test)\n",
    "    ca.append(score)\n",
    "    print('Performance on test with {} nearest neighbours: {:.3f}'.format(k, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "The model generalisation appears to improve as k increases...but only up to a point. We assume this because accuracy increases and model complexity is decreasing (why?). However, as k gets too large, we end up predicting the mean class for every data point (why?!) and performance drops considerably.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.10 ==========\n",
    "Plot the results (k-value on the x-axis and classification accuracy on the y-axis), making sure to mark the axes. Can you conclude anything from observing the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student needs to provide code similar to below\n",
    "plt.scatter(k_list, ca)\n",
    "plt.plot(k_list,ca, )\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Classification accuracy on test set')\n",
    "plt.title('K-nearest neighbours classification')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "Increasing K improves performance up to a certain point. Beyond that point, the classifier uses virtually all training samples to classify a new instance and as a results classifies all instances to the dominant class (identically to our dumb baseline model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.11 ==========\n",
    "Select best value for `k` from Questions 2.9 and 2.10 and plot the normalised confusion matrix on the test set (you may use the provided function). Then plot the confusion matrix for a 5-nearest neighbour classifier. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True)\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student needs to provide code similar to below\n",
    "for i in [1000, 5]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    cm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm_norm, classes=knn.classes_, title = str(i) + ' neighbours')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "With 1000 neighbours, the `N` class is almost always predicted correctly, but the other two classes are not. With fewer neightbors the `EI` class is predicted much better. The 5 neighbours solution is a more 'complex model' (why?) and the mean class accuracy seems to be worse. The choice of model will depend on your goals. If you don't care about predicting `IE` correctly, the `1000` neighbour model gives greater overall accuracy and is a simpler model - therefore one would prefer to err towards using it. However, your application may tolerate a drop in overall accuracy in return for better 'class accuracy' in a particular output - in this case, you may consider the more complex 5 neighbour model to get better `EI` accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.12 ==========\n",
    "Read about the [logarithimic loss](http://scikit-learn.org/0.19/modules/generated/sklearn.metrics.log_loss.html) (or cross-entropy loss). It is often the error metric used when we are trying to optimise classification models.\n",
    "\n",
    "This metric takes as input the true labels and the estimated probability distributions (bernouli or multinomial). It makes sense to use this metric when we are interested not only in the predicted labels, but also in the confidence with which these labels are predicted.\n",
    "\n",
    "For instance, think of the situation where you have a single test point and two classifiers. Both classifiers predict the label correctly, however classifier A predicts that the test point belongs to the class with probability 0.55, whereas classifier B predicts the correct class with probability 0.99. Classification accuracy would be the same for the two classifiers (why?) but the `log_loss` metric would indicate that classifier B should be favoured.\n",
    "\n",
    "Produce a scatter plot similar to the one in Question 2.10 but this time show `log_loss` on your y axis. Which value for `k` would you pick if `log_loss` was the error metric? Comment on why this might happen, and which metric would be a better evaluator of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Student needs to provide code similar to below\n",
    "k_list = [5, 10, 50, 100, 200, 500, 1000, 1500, 2000]\n",
    "logloss = []\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k).fit(X_train, y_train)\n",
    "    pred_proba = knn.predict_proba(X_test)\n",
    "    this_logloss = log_loss(y_test, pred_proba)\n",
    "    logloss.append(this_logloss)\n",
    "    print('Performance on test with ', k, ' nearest neighbours: ', this_logloss)\n",
    "plt.scatter(k_list, logloss)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Logarithmic loss on test set')\n",
    "plt.title('K-nearest neighbours classification')\n",
    "\n",
    "# Extension - plot baselines too\n",
    "baselines = ['prior', 'most_frequent']\n",
    "fig, ax = plt.subplots(len(baselines), 1, figsize=(5.5,6))\n",
    "from sklearn.dummy import DummyClassifier\n",
    "bl_loss = {}\n",
    "for ii, baseline in enumerate(baselines):\n",
    "    plt.sca(ax[ii])\n",
    "    dummy_classifier = DummyClassifier(strategy=baseline).fit(X_train, y_train)\n",
    "    pred_proba = dummy_classifier.predict_proba(X_test)\n",
    "    this_logloss = log_loss(y_test, pred_proba)\n",
    "    plt.scatter(k_list, logloss)\n",
    "    plt.axhline(this_logloss, label='{} baseline'.format(baseline), linestyle='--')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Logarithmic loss on test set')\n",
    "    plt.legend()\n",
    "\n",
    "plt.suptitle('K-nearest neighbours classification')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=.9)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "We would pick the 10-neighbours classifier as it has the lowest generalisation log loss. \n",
    "\n",
    "This is significantly different from the accuracy metric. One way to explain this is as follows. The 10-NN model, while it gets more instances wrong (lower accuracy), has a better judgement of its confidence in the prediction, whereas the 1000-NN model gets more instances right (higher accuracy), but its judgement is way off (for example, in instances where it gets it wrong, it may predict the wrong label with high confidence, impacting the log loss).\n",
    "\n",
    "However, the point where the knn model = dumb baseline is note indicated as clearly on this log loss graph. There are a couple of different dummy classifier baselines we could use for log loss:\n",
    "\n",
    "1. Predict the dominant class with probability of 1 (sklearn.dummy.DummyClassifier(strategy='most_frequent').fit(X_train, y_train))\n",
    "2. Predict probabilites respecting the class distribution from the training dataset (i.e. use prior probabilities) (sklearn.dummy.DummyClassifier(strategy='prior').fit(X_train, y_train))\n",
    "\n",
    "I plotted these baselines on a second set of graphs with dotted lines. As you can see - predicting with 100% confidence comes at a very high cost (in terms of logarithmic loss)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 2.13 ==========\n",
    "\n",
    "Could you use the `log_loss` metric to evaluate the performance of an SVM classifier? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Student needs to answer similar to below:***\n",
    "\n",
    "You can, but you probably shouldn't! SVMs are not probabilistic classifiers (i.e. they only yield a classification decision and not a posterior probability distribution) - they are trained to minimise a distance metric. Other classifiers, such as logistic regression, **explicitly try to minimise the logarithmic loss**. It would make sense that these models would report a better log loss than an SVM, but the SVM could still be producing 'better' predictions e.g. rankings.\n",
    "\n",
    "Nevertheless `sklearn` does provide a `predict_proba()` method which returns probability estimates on predictions by using cross-validation. You can also calibrate the predicted probabilities such that they perform better by the log_loss metric. See [here](http://scikit-learn.org/0.19/modules/calibration.html) for more information.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
